# -*- coding: utf-8 -*-

# --- Импорт необходимых библиотек ---

# `re` - для работы с регулярными выражениями. Используется для "токенизации" - разбиения текста на слова.
import re
# `defaultdict`, `Counter`, `set` - из модуля `collections`.
# `defaultdict` - удобный словарь для построения индекса.
# `Counter` - сверхэффективный инструмент для подсчета частоты слов в документе.
# `set` - структура для хранения уникальных элементов.
from collections import defaultdict, Counter
# `time` - для измерения производительности и демонстрации скорости поиска.
import time
# `sys`, `io` - для настройки терминала, чтобы он корректно отображал русские символы.
import sys
import io
# `os` - для работы с файловой системой (создание папок, формирование путей к файлам).
import os
# `pandas` - для создания таблиц и сохранения результатов в формате Excel.
import pandas as pd
# `json` - для сохранения результатов в формате JSON.
import json

# *** РЕКОМЕНДАЦИЯ №1 и №2: Лемматизация ***
# Для качественной обработки русского языка нам понадобится библиотека pymorphy2.
# Она приводит слова к их словарной форме (лемме): "сервера", "сервером" -> "сервер".
# Установка: pip install pymorphy2
try:
    from pymorphy2 import MorphAnalyzer
except ImportError:
    print("Ошибка: библиотека pymorphy2 не установлена. Пожалуйста, установите ее командой: pip install pymorphy2")
    sys.exit(1)


# --- Настройка кодировки для вывода в терминал ---
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


# --- 1. ИСХОДНЫЕ ДАННЫЕ И ГЛОБАЛЬНЫЕ НАСТРОЙКИ ---

# Это наш "игрушечный" набор данных для демонстрации.
# Ключ - уникальный ID документа, значение - его текстовое содержимое.
DOCUMENTS = {
    "doc_1_vm": "создание сервера в панели управления. для автоматизации используйте api или terraform.",
    "doc_2_k8s": "управляемый kubernetes. создание кластера через api. высокая доступность и мониторинг.",
    "doc_3_s3": "объектное хранилище s3. стоимость хранения и api запросов. есть cli для работы с файлами.",
    "doc_4_db": "управляемые базы данных postgresql. стоимость зависит от конфигурации. резервное копирование.",
    "doc_5_billing": "информация про биллинг и стоимость услуг. оплата по факту использования."
}
# ---
# КОММЕНТАРИЙ: Способы загрузки входных данных (альтернативы)
# В реальной системе данные не были бы прописаны в коде. Основные способы их получения:
# 1. Чтение из папки с файлами (.txt, .html), скачанными ранее (как в Части 1).
# 2. Загрузка из базы данных (например, PostgreSQL), где хранится контент.
# 3. Прямое скачивание с веб-сайтов по списку URL или sitemap.xml (веб-краулинг).
# ---

# *** РЕКОМЕНДАЦИЯ №3: Фильтрация стоп-слов ***
# Создаем множество (set) стоп-слов. Это часто встречающиеся слова, не несущие смысловой нагрузки.
# Их удаление уменьшает размер индекса и повышает релевантность поиска.
# Использование `set` вместо `list` дает мгновенную проверку на наличие слова.
STOP_WORDS = {
    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то',
    'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за',
    'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет',
    'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если',
    'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять',
    'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они',
    'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была'
}

# Настройки для сохранения файлов с результатами.
ANALYSIS_RESULTS_DIR = "analysis_results"
# *** ИЗМЕНЕНИЕ: Скорректированы имена файлов ***
JSON_RESULTS_FILE = os.path.join(ANALYSIS_RESULTS_DIR, "task2_inverted_index.json")
EXCEL_RESULTS_FILE = os.path.join(ANALYSIS_RESULTS_DIR, "task2_inverted_index.xlsx")

# Инициализируем морфологический анализатор. Делаем это один раз глобально,
# так как создание этого объекта - ресурсоемкая операция.
morph = MorphAnalyzer()


# --- 2. УЛУЧШЕННЫЕ ФУНКЦИИ ПОИСКОВОГО ДВИЖКА ---

def tokenize_and_lemmatize(text: str) -> list[str]:
    """
    Улучшенная функция для обработки текста:
    1. Разбивает текст на слова (токены).
    2. Приводит слова к нижнему регистру.
    3. Удаляет стоп-слова.
    4. Приводит каждое слово к его нормальной (словарной) форме - лемме.
    """
    text = text.lower()
    # 1. Находим все последовательности букв/цифр (токены).
    tokens = re.findall(r'\b[a-zа-я0-9]+\b', text)
    lemmas = []
    for token in tokens:
        # 2. Проверяем, не является ли токен стоп-словом.
        if token not in STOP_WORDS:
            # 3. Приводим слово к нормальной форме и добавляем в список.
            lemmas.append(morph.parse(token)[0].normal_form)
    return lemmas


def build_rich_inverted_index(docs: dict[str, str]) -> defaultdict[str, dict[str, int]]:
    """
    Создает "Обогащенный обратный индекс".
    В отличие от простой версии, он хранит не только факт наличия слова,
    но и частоту его употребления (TF - Term Frequency) в документе.
    Это необходимо для ранжирования результатов.
    Структура: { "лемма": { "doc_id_1": частота, "doc_id_2": частота } }
    """
    # Внешний словарь: ключ - лемма, значение - внутренний словарь.
    # Внутренний словарь: ключ - ID документа, значение - частота леммы в нем.
    inverted_index = defaultdict(dict)
    for doc_id, text in docs.items():
        # Получаем список лемм для документа.
        lemmas = tokenize_and_lemmatize(text)
        # С помощью Counter мгновенно считаем, сколько раз каждая лемма встретилась в тексте.
        # Например: ['создание', 'сервер', 'api'] -> {'создание': 1, 'сервер': 1, 'api': 1}
        lemma_counts = Counter(lemmas)
        
        for lemma, count in lemma_counts.items():
            # Заполняем наш индекс: для этой леммы, в этом документе, частота равна count.
            inverted_index[lemma][doc_id] = count
            
    return inverted_index


def search_and_rank(query: str, index: defaultdict[str, dict[str, int]]) -> list[tuple[str, int]]:
    """
    Выполняет поиск по "обогащенному" индексу и ранжирует результаты.
    1. Находит документы, где есть ВСЕ слова из запроса.
    2. Сортирует найденные документы по релевантности.
    """
    # Обрабатываем поисковый запрос так же, как и тексты документов.
    query_lemmas = tokenize_and_lemmatize(query)
    if not query_lemmas:
        return []

    # --- Шаг 1: Поиск документов (пересечение) ---
    # Находим документы, в которых есть ПЕРВАЯ лемма из запроса.
    # `.keys()` возвращает ID всех документов, где есть лемма.
    try:
        result_doc_ids = set(index[query_lemmas[0]].keys())
    except KeyError:
        # Если даже первой леммы нет в индексе, то результатов точно нет.
        return []

    # Последовательно "отсекаем" документы, в которых нет ОСТАЛЬНЫХ лемм.
    # `intersection_update` - очень быстрая операция над множествами.
    for lemma in query_lemmas[1:]:
        if lemma in index:
            result_doc_ids.intersection_update(index[lemma].keys())
        else:
            # Если хотя бы одной леммы из запроса нет в индексе, результатов не будет.
            return []
    
    if not result_doc_ids:
        return []

    # --- Шаг 2: Ранжирование найденных документов ---
    # *** РЕКОМЕНДАЦИЯ №2: Ранжирование результатов ***
    # Создаем словарь, где будем хранить "очки релевантности" для каждого найденного документа.
    doc_scores = defaultdict(int)
    # Простейший, но эффективный способ ранжирования:
    # Релевантность документа = сумма частот всех слов из запроса в этом документе.
    for doc_id in result_doc_ids:
        score = 0
        for lemma in query_lemmas:
            # Берем из индекса, как часто эта лемма встречается в этом документе, и прибавляем к очкам.
            score += index[lemma][doc_id]
        doc_scores[doc_id] = score
        
    # Сортируем документы по их очкам релевантности (по убыванию).
    sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)
    
    return sorted_docs


# --- 3. ОСНОВНОЙ БЛОК ДЕМОНСТРАЦИИ ---
if __name__ == "__main__":
    print("--- Симуляция поиска с лемматизацией и ранжированием ---")
    
    # --- ЭТАП 1: ИНДЕКСАЦИЯ ---
    print("\n[ЭТАП 1] Построение обогащенного обратного индекса...")
    start_time = time.perf_counter()
    search_index = build_rich_inverted_index(DOCUMENTS)
    end_time = time.perf_counter()
    print(f"✅ Индекс для {len(DOCUMENTS)} документов построен за {end_time - start_time:.6f} секунд.")
    
    print("\nПример части построенного индекса (Лемма -> {Документ: Частота}):")
    print(f"  Лемма 'api' -> {search_index.get('api')}")
    print(f"  Лемма 'kubernetes' -> {search_index.get('kubernetes')}")
    print(f"  Лемма 'стоимость' -> {search_index.get('стоимость')}")

    # --- ЭТАП 2: ЭКСПОРТ ИНДЕКСА В ФАЙЛЫ (ОПЦИОНАЛЬНО) ---
    print("\n[ЭТАП 2] Экспорт индекса в файлы для анализа...")

    if not os.path.exists(ANALYSIS_RESULTS_DIR):
        os.makedirs(ANALYSIS_RESULTS_DIR)

    with open(JSON_RESULTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(search_index, f, indent=2, ensure_ascii=False)
    print(f"✅ Индекс сохранен в формате JSON: {JSON_RESULTS_FILE}")

    csv_data = []
    for lemma, doc_freqs in search_index.items():
        for doc_id, frequency in doc_freqs.items():
            csv_data.append({'lemma': lemma, 'document_id': doc_id, 'frequency': frequency})
    pd.DataFrame(csv_data).to_excel(EXCEL_RESULTS_FILE, index=False)
    print(f"✅ Индекс сохранен в формате Excel: {EXCEL_RESULTS_FILE}")

    # --- ЭТАП 3: ПОИСК С РАНЖИРОВАНИЕМ ---
    print("\n[ЭТАП 3] Выполнение поисковых запросов по индексу...\n")

    queries_to_test = [
        "API и стоимость",      # Найдет doc_3, т.к. там есть оба слова
        "создание сервера",     # Найдет doc_1, благодаря лемматизации ("сервера" -> "сервер")
        "создание api",         # Найдет doc_1 и doc_2.
        "управляемый kubernetes", # Найдет doc_2
        "документ про биллинг", # Найдет doc_5, "про" будет отфильтровано как стоп-слово
        "terraform"             # Найдет doc_1
    ]
    # ---
    # КОММЕНТАРИЙ: Способы ввода поисковых запросов (альтернативы)
    # В реальном приложении запросы не были бы прописаны в коде. Основные способы их получения:
    # 1. Интерактивный ввод в консоли: с помощью цикла `while True` и функции `input()`.
    # 2. Аргументы командной строки: скрипт запускается как `python script.py "мой запрос"`.
    # 3. Веб-интерфейс: через API-запрос от сайта с полем поиска (промышленный стандарт).
    # ---

    for q in queries_to_test:
        start_time = time.perf_counter()
        search_results = search_and_rank(q, search_index)
        end_time = time.perf_counter()
        
        print(f"> Поисковый запрос: '{q}'")
        print(f"  Найдено за {(end_time - start_time):.8f} секунд.")
        if search_results:
            # Выводим результаты в формате (документ, очки релевантности)
            print(f"  Результаты (документ, релевантность): {search_results}")
        else:
            print("  Ничего не найдено.")
        print("-" * 40)

    # --- ЭТАП 4: ВЫВОДЫ И МАСШТАБИРОВАНИЕ ---
    # *** РЕКОМЕНДАЦИЯ №4: Куда двигаться дальше? ***
    print("\n[ЭТАП 4] Выводы по масштабированию")
    print("""
Эта симуляция демонстрирует фундаментальные принципы работы современного поиска.
Однако для работы с миллионами документов (в масштабах всего веба или крупной компании)
индекс перестает помещаться в оперативную память одной машины.
    
Для таких задач используют специализированные поисковые движки, такие как:
  - Elasticsearch
  - OpenSearch
  
Они решают проблемы хранения индекса на диске, его распределения по кластеру серверов,
отказоустойчивости, а также предоставляют сложнейшие алгоритмы ранжирования (TF-IDF, BM25)
и анализа текста "из коробки". Этот скрипт является отличной базой для понимания того,
что происходит "под капотом" у этих гигантов.
    """)
    
    print("🎉 Демонстрация завершена.")